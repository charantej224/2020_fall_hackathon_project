Clustering problems involve data to be divided into subsets. These subsets, also called clusters, contain data that are similar to each other.
Different clusters reveal different details about the objects, unlike classification or regression.
There are two techniques used in unsupervised learning: clustering and association.
Clustering is very much important as it determines the intrinsic grouping among the unlabeled data present.
There are no criteria for a good clustering. It depends on the user, what is the criteria they may use which satisfy their need.
For instance, we could be interested in finding representatives for homogeneous groups (data reduction), in finding “natural clusters” and describe their unknown properties (“natural” data types), in finding useful and suitable groupings (“useful” data classes) or in finding unusual data objects (outlier detection).
This algorithm must make some assumptions which constitute the similarity of points and each assumption make different and equally valid clusters.
1. Difference between Density-based and Distributed-based clustering algorithms?
2. What is the clustering algorithm developed based on Centroid-based?


Types of clustering algorithms:There are different types of clustering algorithms that handle all kinds of unique data.

1. Density-based
In density-based clustering, data is grouped by areas of high concentrations of data points surrounded by areas of low concentrations of data points.
Basically the algorithm finds the places that are dense with data points and calls those clusters.
The great thing about this is that the clusters can be any shape. You aren't constrained to expected conditions.
The clustering algorithms under this type don't try to assign outliers to clusters, so they get ignored.
OPTICS stands for Ordering Points to Identify the Clustering Structure. It's a density-based algorithm similar to DBSCAN,
but it's better because it can find meaningful clusters in data that varies in density.

2. Distribution-based
With a distribution-based clustering approach, all of the data points are considered parts of a cluster based on the probability that they belong to
a given cluster. It works like this: there is a center-point, and as the distance of a data point from the center increases, the probability of it
being a part of that cluster decreases. If you aren't sure of how the distribution in your data might be, you should consider a different type of
algorithm.

3. Centroid-based
Centroid-based clustering is the one you probably hear about the most. It's a little sensitive to the initial parameters you give it, but it's fast and
efficient. These types of algorithms separate data points based on multiple centroids in the data. Each data point is assigned to a
cluster based on its squared distance from the centroid. This is the most commonly used type of clustering. K-means clustering is the most commonly used
clustering algorithm. It's a centroid-based algorithm and the simplest unsupervised learning algorithm

4. Hierarchical-based
Hierarchical-based clustering is typically used on hierarchical data, like you would get from a company database or taxonomies. It builds a tree of
clusters so everything is organized from the top-down. This is more restrictive than the other clustering types, but it's perfect for specific
kinds of data sets

Clustering is especially useful for exploring data you know nothing about. It might take some time to figure out which type of clustering algorithm
works the best, but when you do, you'll get invaluable insight on your data. You might find connections you never would have thought of.  Some real world
applications of clustering include fraud detection in insurance, categorizing books in a library, and customer segmentation in marketing. It can also be used in larger problems,
like earthquake analysis or city planning. Clustering has a large no. of applications spread across various domains. Some of the most popular applications of clustering are:
Recommendation engines, Market segmentation, Social network analysis, Search result grouping, Medical imaging, Image segmentation and Anomaly detection.


The following are few limitations with K-Means clustering:
Sometimes, it is quite tough to forecast the number of clusters, or the value of k.
The output is highly influenced by original input, for example, the number of clusters.
An array of data substantially hits the concluding outcomes.
In some cases, clusters show complex spatial views, then executing clustering is not a good choice.
Also, rescaling is sometimes conscious, it can’t be done by normalization or standardization of data points, the output gets changed entirely.
