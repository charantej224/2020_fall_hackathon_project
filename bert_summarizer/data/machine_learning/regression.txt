There are four techniques to evaluate the performance of the regression algorithm such as Mean Absolute Error (MAE),
Mean Square Error (MSE), Suma of Squared(SSE) and  R Squared.
Linear regression, Logistic regression, Ridge regression, Lasso regression, Polynomial regression, stepwise regression and ElasticNet regression.
In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship
between the independent and dependent variables. In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit
relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.
One can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so.
Time series data is mostly used for the prediction of the future, but linear regression seldom gives good results for future prediction as it is not meant for extrapolation.
Mostly, time series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis.
An outlier is an observation point distant from other observations. It might be due to a variance in the measurement. It can also indicate an experimental
error. Under such circumstances, you need to exclude the same from the data set. If you do not detect and treat them, they can cause problems in
statistical analysis. There is no strict mathematical calculation of how to determine an outlier. Deciding whether an observation is an outlier or not,
is itself a subjective exercise. However, you can detect outliers through various methods. Some of them are graphical and are known as normal probability
plots whereas some are model-based. You have some hybrid techniques such as Box plots. Once you have detected the outlier, you should either remove them or
correct them to ensure accurate analysis. Some of the methods of eliminating outliers are the Z-Score and the IQR Score methods.
It is also called simple linear regression. It establishes the relationship between two variables using a straight line. Linear regression attempts to
draw a line that comes closest to the data by finding the slope and intercept that define the line and minimize regression errors.
It is rare that a dependent variable is explained by only one variable. In this case, an analyst uses multiple regression, which attempts to
explain a dependent variable using more than one independent variable. Multiple regressions can be linear and nonlinear.

