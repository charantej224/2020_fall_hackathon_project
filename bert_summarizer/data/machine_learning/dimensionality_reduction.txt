In Machine Learning and statistics, dimension reduction is the process of reducing the number of random variables under considerations and
can be divided into feature selection and feature extraction. Dimension Reduction is the process of reducing the size of the feature matrix.
We try to reduce the number of columns so that we get a better feature set either by combining columns or by removing extra variables.
In traditional dataset, the dimensions(p) of the data was low with many observations(n). In this case , classical rules such as the Central Limit Theorem
are often applied to obtain some inference from data. A new challenge today is dealing with a different setting, when the data dimension(p) is very large
and the number of observations(n) is small. So it means that whenever a dataset has 'p' is more than 'n' then it is high dimensional data. Issues with
high dimensional data sets leads to overfitting, vast time and space complexity and not all features are relevant to our problem.
Principal Component Analysis (PCA)
Linear Discriminant Analysis (LDA)
Generalized Discriminant Analysis (GDA)
There are various methods used for dimensionality reduction but the most prevalent method in practice is : Principal Component Analysis (PCA).
PCA is one of the most successful high dimensional analytics technique. This method was introduced by Karl Pearson.
PCA seeks to explain the correlation structure of a set of predictor variables, using a smaller set of linear combinations of these variables.
These linear combinations are called components.It means that the total variability of a data set produced by the complete set of m variables
can be accounted for by a smaller set of k linear combinations and there is almost as much information in the k components as there is in the original m variables.
Dimensionality reduction is the process of reducing the number of random variables under review, by obtaining a set of principal components.
That is the data is converted from a high dimensional space into lower number of dimensions without loosing much of information.
An example of dimensionality reduction can be discussed through a simple digital e-mail marketing classification problem, where we need to classify
whether the e-mail is spam or not. This can involve a large number of features, such as whether or not the e-mail has a standard title,
the content of the e-mail, whether the e-mail uses a personalised template, etc.Hence, we can reduce the number of features in such problems.
A 3-D classification problem can be hard to visualize, whereas a 2-D one can be mapped to a simple 2 dimensional space, and a 1-D problem to a simple line.
There are two components of dimensionality reduction Feature selection and Feature extraction.
Feature selection: In this, we try to find a subset of the original set of variables, or features, to get a smaller subset which can be used to model the problem.
Dimensionality Reduction helps in data compression, and hence reduced storage space.
It reduces computation time.
It also helps remove redundant features, if any.
Dimensionality Reduction helps in data compressing and reducing the storage space required
It fastens the time required for performing the same computations.
If there present fewer dimensions then it leads to less computing. Also, dimensions can allow usage of algorithms unfit for a large number of dimensions.
It takes care of multicollinearity that improves model performance. It removes redundant features.

